{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting and training the model\n",
    "This was mostly done in Colab with T4 GPU, but can be run on CPU as well it will just take longer.\n",
    "Either way you need CUDA toolkit installed. <br>\n",
    "I used a template from MONAI and modified it to fit my needs, the original can be found here: https://github.com/Project-MONAI/tutorials/blob/main/2d_segmentation/torch/unet_training_dict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make all the necesarry imports\n",
    "import logging\n",
    "import sys\n",
    "import tempfile\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import monai\n",
    "from monai.utils import set_determinism\n",
    "from monai.data import (\n",
    "    decollate_batch, \n",
    "    DataLoader, \n",
    "    pad_list_data_collate\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    RandRotate90d,\n",
    "    ScaleIntensityd,\n",
    "    RandSpatialCropSamplesd,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set determinism fro reproducibility\n",
    "set_determinism(seed=42)\n",
    "\n",
    "# Set the logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "# Set random seed\n",
    "random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the controls are missing ground thruths we need to create the ground thruths for them. <br>\n",
    "We create the images with the same size as the controls and fill them with zeros. <br>\n",
    "\n",
    "***(Edit the paths to where you have the data)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to healthy scans\n",
    "healthy_images = glob.glob('../data/controls/imgs/*.png')\n",
    "healthy_images.sort()\n",
    "\n",
    "# Create empty ground truths for healthy images\n",
    "for img_path in healthy_images:\n",
    "    img = Image.open(img_path)\n",
    "    empty_gt = np.zeros((img.size[1], img.size[0], 4), dtype=np.uint8)  # Adjusting to the image dimensions\n",
    "    empty_gt[..., 3] = 255 # Set the alpha channel to 255 so they are black\n",
    "    empty_gt_path = img_path.replace('../data/controls/imgs/', '../data/controls/lables/')\n",
    "    Image.fromarray(empty_gt).save(empty_gt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'img': '../data/patients/imgs/patient_000.png', 'seg': '../data/patients/labels/segmentation_000.png'}, {'img': '../data/patients/imgs/patient_001.png', 'seg': '../data/patients/labels/segmentation_001.png'}, {'img': '../data/patients/imgs/patient_002.png', 'seg': '../data/patients/labels/segmentation_002.png'}, {'img': '../data/patients/imgs/patient_003.png', 'seg': '../data/patients/labels/segmentation_003.png'}, {'img': '../data/patients/imgs/patient_004.png', 'seg': '../data/patients/labels/segmentation_004.png'}]\n"
     ]
    }
   ],
   "source": [
    "# Make a list of healthy images\n",
    "healthy_images = glob.glob('../data/controls/imgs/*.png')\n",
    "healthy_images.sort()\n",
    "sampled_healthy_images = random.sample(healthy_images, 182)\n",
    "\n",
    "# Make a list of healthy ground truths\n",
    "sampled_healthy_gt = [img_path.replace('../data/controls/imgs/', '../data/controls/lables/') for img_path in sampled_healthy_images]\n",
    "\n",
    "# Make a list of patient images\n",
    "patient_images = glob.glob('../data/patients/imgs/*.png') \n",
    "patient_images.sort()\n",
    "\n",
    "# Make a list of patient ground truths\n",
    "patient_gt = glob.glob('../data/patients/labels/*.png')\n",
    "patient_gt.sort()\n",
    "\n",
    "# Combine patient and sampled healthy data\n",
    "all_images = patient_images + sampled_healthy_images\n",
    "all_gts = patient_gt + sampled_healthy_gt\n",
    "\n",
    "# Ensure each image has a corresponding label\n",
    "assert len(all_images) == len(all_gts)\n",
    "\n",
    "# Create a list of dictionaries\n",
    "data_dicts = [{'img': img, 'seg': gt} for img, gt in zip(all_images, all_gts)]\n",
    "\n",
    "print(data_dicts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to training and validation set\n",
    "train_files, val_files = train_test_split(data_dicts, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation pipeline\n",
    "Using MONAI's pipeline to transform the data. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for training data\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "        EnsureChannelFirstd(keys=[\"img\", \"seg\"]),\n",
    "        ScaleIntensityd(keys=[\"img\", \"seg\"]),\n",
    "        RandSpatialCropSamplesd(\n",
    "            keys=[\"img\", \"seg\"], roi_size=[96, 96], num_samples=4,\n",
    "        ),\n",
    "        RandRotate90d(keys=[\"img\", \"seg\"], prob=0.5, spatial_axes=[0, 1]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define transforms for validation data\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "        EnsureChannelFirstd(keys=[\"img\", \"seg\"]),\n",
    "        ScaleIntensityd(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the data loaders for training and validation. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 4, 96, 96]) torch.Size([24, 4, 96, 96])\n"
     ]
    }
   ],
   "source": [
    "# First check that the training transforms are working\n",
    "check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=6, num_workers=2, collate_fn=pad_list_data_collate)\n",
    "check_data = monai.utils.misc.first(check_loader)\n",
    "print(check_data[\"img\"].shape, check_data[\"seg\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training data loader\n",
    "train_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "# Use batch_size=3 to load images and use RandSpatialCropSamplesd to generate 3 x 4 images for network training\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    num_workers=2, # can be set to higher if you have more cores\n",
    "    collate_fn=pad_list_data_collate,\n",
    "    pin_memory=torch.cuda.is_available(),\n",
    ")\n",
    "# Create a validation data loader\n",
    "val_ds = monai.data.Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=6, \n",
    "    num_workers=2, \n",
    "    collate_fn=pad_list_data_collate\n",
    ")\n",
    "\n",
    "# Set the metrics for evaluation\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "I used a `AttentionUNet` architecture, a model from the Monai library. This model's concept is originally introduced in a research paper, which can be accessed at: https://arxiv.org/abs/1804.03999 <br>\n",
    "I aligned my approach with recommendations from another study on whole-body MIP-PET imaging, detailed in a paper available here: https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.16438. Following the guidelines from this paper, I used the `NAdam` optimizer along with the `DiceFocal` loss function for training. <br>\n",
    "\n",
    "\n",
    "The model is set up to use the GPU if available, if not it will use the CPU. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AttentionUNet\n",
    "model = monai.networks.nets.AttentionUnet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(device)\n",
    "\n",
    "# Set the loss function, optimizer and scheduler\n",
    "loss_function = monai.losses.DiceFocalLoss(sigmoid=True, lambda_dice=1, lambda_focal=10)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), 1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(tempdir, epochs=10):\n",
    "    monai.config.print_config()\n",
    "    val_interval = 2\n",
    "    best_metric = -1\n",
    "    best_metric_epoch = -1\n",
    "    epoch_loss_values = list()\n",
    "    metric_values = list()\n",
    "    writer = SummaryWriter()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"-\" * epochs)\n",
    "        print(f\"epoch {epoch}/{epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        step = 0\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs, labels = batch_data[\"img\"].to(device), batch_data[\"seg\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_len = len(train_ds) // train_loader.batch_size\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "        epoch_loss /= step\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_images = None\n",
    "                val_labels = None\n",
    "                val_outputs = None\n",
    "                for val_data in val_loader:\n",
    "                    val_images, val_labels = val_data[\"img\"].to(device), val_data[\"seg\"].to(device)\n",
    "                    roi_size = (96, 96)\n",
    "                    sw_batch_size = 4\n",
    "                    val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n",
    "                    val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                    # compute metric for current iteration\n",
    "                    dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                # aggregate the final mean dice result\n",
    "                metric = dice_metric.aggregate().item()\n",
    "                scheduler.step(metric)\n",
    "                # reset the status for next validation round\n",
    "                dice_metric.reset()\n",
    "                metric_values.append(metric)\n",
    "                if metric > best_metric:\n",
    "                    best_metric = metric\n",
    "                    best_metric_epoch = epoch + 1\n",
    "                    torch.save(model.state_dict(), \"best_metric_model_segmentation2d_dict.pth\")\n",
    "                    print(\"saved new best metric model\")\n",
    "                print(\n",
    "                    \"current epoch: {} current mean dice: {:.4f} best mean dice: {:.4f} at epoch {}\".format(\n",
    "                        epoch + 1, metric, best_metric, best_metric_epoch\n",
    "                    )\n",
    "                )\n",
    "                writer.add_scalar(\"val_mean_dice\", metric, epoch + 1)\n",
    "                # plot the last model output as GIF image in TensorBoard with the corresponding image and label\n",
    "                plot_2d_or_3d_image(val_images, epoch + 1, writer, index=0, tag=\"image\")\n",
    "                plot_2d_or_3d_image(val_labels, epoch + 1, writer, index=0, tag=\"label\")\n",
    "                plot_2d_or_3d_image(val_outputs, epoch + 1, writer, index=0, tag=\"output\")\n",
    "\n",
    "    print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the number of epochs and run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tempdir:\n",
    "  training(tempdir, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation metric: 0.8637876510620117\n"
     ]
    }
   ],
   "source": [
    "from monai.transforms import SaveImage\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(\"/home/magsam/workspace/tumor-segmentation/src/model/best_metric_model_segmentation2d_dict (4).pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Sliding window inference need to input 1 image in every iteration\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=2, collate_fn=pad_list_data_collate)\n",
    "saver = SaveImage(output_dir=\"./output\", output_ext=\".png\", output_postfix=\"seg\")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_loader:\n",
    "        val_images, val_labels = val_data[\"img\"].to(device), val_data[\"seg\"].to(device)\n",
    "        # define sliding window size and batch size for windows inference\n",
    "        roi_size = (96, 96)\n",
    "        sw_batch_size = 4\n",
    "        val_outputs = sliding_window_inference(val_images, roi_size, sw_batch_size, model)\n",
    "        val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "        val_labels = decollate_batch(val_labels)\n",
    "        # compute metric for current iteration\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        #for val_output in val_outputs:\n",
    "            #saver(val_output)\n",
    "    # aggregate the final mean dice result\n",
    "    print(\"evaluation metric:\", dice_metric.aggregate().item())\n",
    "    # reset the status\n",
    "    dice_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumor-segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
